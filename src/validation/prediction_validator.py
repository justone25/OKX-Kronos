#!/usr/bin/env python3
"""
Kronosé¢„æµ‹éªŒè¯ç³»ç»Ÿ
åŸºäºæ—¶é—´åºåˆ—é¢„æµ‹è¯„ä¼°æ ‡å‡†ï¼ŒéªŒè¯Kronosé¢„æµ‹çš„å‡†ç¡®æ€§
"""
import os
import logging
import sqlite3
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
from enum import Enum

from ..data.okx_fetcher import OKXDataFetcher
from ..utils.config import OKXConfig


class ValidationStatus(Enum):
    """éªŒè¯çŠ¶æ€"""
    PENDING = "ç­‰å¾…éªŒè¯"        # ç­‰å¾…éªŒè¯
    VALIDATED = "å·²éªŒè¯"       # å·²éªŒè¯
    EXPIRED = "å·²è¿‡æœŸ"         # å·²è¿‡æœŸ
    FAILED = "éªŒè¯å¤±è´¥"        # éªŒè¯å¤±è´¥
    EXCELLENT = "ä¼˜ç§€"         # ä¼˜ç§€é¢„æµ‹
    GOOD = "è‰¯å¥½"              # è‰¯å¥½é¢„æµ‹
    FAIR = "ä¸€èˆ¬"              # ä¸€èˆ¬é¢„æµ‹
    POOR = "è¾ƒå·®"              # è¾ƒå·®é¢„æµ‹


@dataclass
class ValidationResult:
    """éªŒè¯ç»“æœï¼ˆå¢å¼ºç‰ˆï¼šæ”¯æŒé«˜ä½ä»·éªŒè¯ï¼‰"""
    prediction_id: int
    prediction_timestamp: datetime
    validation_timestamp: datetime
    predicted_price: float
    actual_price: float
    actual_high: float = None
    actual_low: float = None
    price_error: float = 0.0
    price_error_pct: float = 0.0
    predicted_direction: str = ""
    actual_direction: str = ""
    direction_correct: bool = False
    high_prediction_correct: bool = False
    low_prediction_correct: bool = False
    confidence_score: float = 0.0
    validation_status: ValidationStatus = ValidationStatus.PENDING

    # è¯„ä¼°æŒ‡æ ‡
    mae: float = 0.0
    rmse: float = 0.0
    mape: float = 0.0
    directional_accuracy: float = 0.0
    confidence_calibration: float = 0.0


class PredictionValidator:
    """Kronosé¢„æµ‹éªŒè¯å™¨"""
    
    def __init__(self, okx_config: OKXConfig, db_path: str = "./data/predictions.db"):
        """
        åˆå§‹åŒ–é¢„æµ‹éªŒè¯å™¨
        
        Args:
            okx_config: OKXé…ç½®
            db_path: é¢„æµ‹æ•°æ®åº“è·¯å¾„
        """
        self.logger = logging.getLogger(__name__)
        self.config = okx_config  # ä¿æŒå‘åå…¼å®¹
        self.okx_config = okx_config
        self.db_path = db_path

        # åˆå§‹åŒ–æ•°æ®è·å–å™¨
        self.data_fetcher = OKXDataFetcher(okx_config)
        
        # åˆå§‹åŒ–éªŒè¯æ•°æ®åº“
        self._init_validation_database()
        
        self.logger.info("Kronosé¢„æµ‹éªŒè¯å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _init_validation_database(self):
        """åˆå§‹åŒ–éªŒè¯æ•°æ®åº“è¡¨"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # åˆ›å»ºéªŒè¯ç»“æœè¡¨
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS prediction_validations (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    prediction_id INTEGER,
                    prediction_timestamp DATETIME,
                    validation_timestamp DATETIME,
                    predicted_price REAL,
                    actual_price REAL,
                    price_error REAL,
                    price_error_pct REAL,
                    predicted_direction TEXT,
                    actual_direction TEXT,
                    direction_correct BOOLEAN,
                    confidence_score REAL,
                    validation_status TEXT,
                    mae REAL,
                    rmse REAL,
                    mape REAL,
                    directional_accuracy REAL,
                    confidence_calibration REAL,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                    FOREIGN KEY (prediction_id) REFERENCES predictions (id)
                )
            ''')
            
            # åˆ›å»ºéªŒè¯ç»Ÿè®¡è¡¨
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS validation_statistics (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    period_start DATETIME,
                    period_end DATETIME,
                    total_predictions INTEGER,
                    validated_predictions INTEGER,
                    avg_mae REAL,
                    avg_rmse REAL,
                    avg_mape REAL,
                    directional_accuracy REAL,
                    confidence_calibration REAL,
                    reliability_score REAL,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            conn.commit()
            conn.close()
            
            self.logger.info("éªŒè¯æ•°æ®åº“è¡¨åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"åˆå§‹åŒ–éªŒè¯æ•°æ®åº“å¤±è´¥: {e}")
            raise
    
    def get_pending_validations(self) -> List[Dict]:
        """è·å–å¾…éªŒè¯çš„é¢„æµ‹"""
        try:
            conn = sqlite3.connect(self.db_path)

            # æŸ¥è¯¢éœ€è¦éªŒè¯çš„é¢„æµ‹ï¼ˆé¢„æµ‹æ—¶é—´å·²åˆ°æœŸä¸”æœªéªŒè¯ï¼‰
            current_time = datetime.now()

            query = '''
                SELECT p.id, p.instrument, p.timestamp, p.current_price, p.predicted_price,
                       p.price_change_pct, p.trend_direction, p.pred_hours,
                       p.volatility,
                       datetime(p.timestamp, '+' || p.pred_hours || ' hours') as target_time
                FROM predictions p
                LEFT JOIN prediction_validations pv ON p.id = pv.prediction_id
                WHERE pv.prediction_id IS NULL
                AND datetime(p.timestamp, '+' || p.pred_hours || ' hours') <= datetime(?)
                AND datetime(p.timestamp, '+' || p.pred_hours || ' hours') >= datetime(?)
                ORDER BY p.timestamp ASC
            '''

            # éªŒè¯çª—å£ï¼šé¢„æµ‹åˆ°æœŸæ—¶é—´åˆ°é¢„æµ‹åˆ°æœŸå30åˆ†é’Ÿ
            # æŸ¥æ‰¾ç›®æ ‡æ—¶é—´åœ¨ [å½“å‰æ—¶é—´-30åˆ†é’Ÿ, å½“å‰æ—¶é—´] èŒƒå›´å†…çš„é¢„æµ‹
            validation_window_start = (current_time - timedelta(minutes=30)).isoformat()
            validation_window_end = current_time.isoformat()

            df = pd.read_sql_query(query, conn, params=(validation_window_end, validation_window_start))

            # åŒæ—¶å¤„ç†è¿‡æœŸçš„é¢„æµ‹ï¼ˆç›®æ ‡æ—¶é—´+30åˆ†é’Ÿ < å½“å‰æ—¶é—´çš„é¢„æµ‹æ ‡è®°ä¸ºè¿‡æœŸï¼‰
            expired_query = '''
                SELECT p.id
                FROM predictions p
                LEFT JOIN prediction_validations pv ON p.id = pv.prediction_id
                WHERE pv.prediction_id IS NULL
                AND datetime(p.timestamp, '+' || p.pred_hours || ' hours', '+30 minutes') < datetime(?)
            '''

            # è¿‡æœŸæ—¶é—´ï¼šå½“å‰æ—¶é—´ï¼ˆç›®æ ‡æ—¶é—´+30åˆ†é’Ÿ < å½“å‰æ—¶é—´ï¼‰
            expired_cutoff = current_time.isoformat()
            expired_df = pd.read_sql_query(expired_query, conn, params=(expired_cutoff,))

            # å°†è¿‡æœŸçš„é¢„æµ‹æ ‡è®°ä¸ºEXPIRED
            if not expired_df.empty:
                expired_ids = expired_df['id'].tolist()
                self.logger.info(f"æ ‡è®° {len(expired_ids)} ä¸ªè¿‡æœŸé¢„æµ‹")

                # æ‰¹é‡æ’å…¥è¿‡æœŸè®°å½•
                cursor = conn.cursor()
                for pred_id in expired_ids:
                    cursor.execute('''
                        INSERT INTO prediction_validations (
                            prediction_id, validation_timestamp, validation_status
                        ) VALUES (?, ?, ?)
                    ''', (pred_id, current_time.isoformat(), 'EXPIRED'))

                conn.commit()

            conn.close()

            return df.to_dict('records')

        except Exception as e:
            self.logger.error(f"è·å–å¾…éªŒè¯é¢„æµ‹å¤±è´¥: {e}")
            return []
    
    def validate_prediction(self, prediction: Dict) -> Optional[ValidationResult]:
        """
        éªŒè¯å•ä¸ªé¢„æµ‹
        
        Args:
            prediction: é¢„æµ‹æ•°æ®
            
        Returns:
            éªŒè¯ç»“æœ
        """
        try:
            prediction_id = prediction['id']
            prediction_timestamp = datetime.fromisoformat(prediction['timestamp'])
            pred_hours = prediction['pred_hours']
            predicted_price = float(prediction['predicted_price'])
            predicted_direction = prediction['trend_direction']
            current_price = float(prediction['current_price'])
            
            # è®¡ç®—éªŒè¯æ—¶é—´ç‚¹
            validation_time = prediction_timestamp + timedelta(hours=pred_hours)
            
            # è·å–éªŒè¯æ—¶é—´ç‚¹çš„å®é™…Kçº¿æ•°æ®
            instrument = prediction.get('instrument', 'BTC-USDT-SWAP')
            kline_data = self._get_actual_kline_at_time(instrument, validation_time)

            if kline_data is None:
                self.logger.warning(f"æ— æ³•è·å–éªŒè¯æ—¶é—´ç‚¹çš„Kçº¿æ•°æ®: {validation_time}")
                return None

            actual_price = kline_data['close']
            actual_high = kline_data['high']
            actual_low = kline_data['low']
            
            # è®¡ç®—ä»·æ ¼è¯¯å·®
            price_error = actual_price - predicted_price
            price_error_pct = (price_error / predicted_price) * 100
            
            # è®¡ç®—å®é™…æ–¹å‘
            actual_direction = self._calculate_actual_direction(current_price, actual_price)
            
            # åˆ¤æ–­æ–¹å‘é¢„æµ‹æ˜¯å¦æ­£ç¡®
            direction_correct = self._is_direction_correct(predicted_direction, actual_direction)
            
            # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
            mae = abs(price_error)
            rmse = price_error ** 2  # å•ä¸ªé¢„æµ‹çš„RMSEå°±æ˜¯è¯¯å·®çš„å¹³æ–¹
            mape = abs(price_error_pct)
            directional_accuracy = 1.0 if direction_correct else 0.0
            
            # è®¡ç®—ç½®ä¿¡åº¦æ ¡å‡†ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
            confidence_calibration = self._calculate_confidence_calibration(
                predicted_price, actual_price, prediction.get('volatility', 0)
            )
            
            # éªŒè¯é«˜ä½ä»·é¢„æµ‹ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            predicted_high = prediction.get('predicted_high')
            predicted_low = prediction.get('predicted_low')

            high_prediction_correct = False
            low_prediction_correct = False

            if predicted_high is not None:
                high_error_pct = abs((actual_high - predicted_high) / predicted_high) * 100
                high_prediction_correct = high_error_pct <= 5.0  # 5%å®¹å·®

            if predicted_low is not None:
                low_error_pct = abs((actual_low - predicted_low) / predicted_low) * 100
                low_prediction_correct = low_error_pct <= 5.0  # 5%å®¹å·®

            # ç¡®å®šéªŒè¯çŠ¶æ€ï¼ˆç»¼åˆè€ƒè™‘ä»·æ ¼ã€æ–¹å‘ã€é«˜ä½ä»·ï¼‰
            price_accuracy = abs(price_error_pct)
            high_low_bonus = 0.0

            if predicted_high is not None and high_prediction_correct:
                high_low_bonus += 0.5
            if predicted_low is not None and low_prediction_correct:
                high_low_bonus += 0.5

            if price_accuracy <= 1.0 and direction_correct:
                validation_status = ValidationStatus.EXCELLENT
            elif price_accuracy <= 2.0 and (direction_correct or high_low_bonus > 0):
                validation_status = ValidationStatus.EXCELLENT
            elif price_accuracy <= 3.0 and direction_correct:
                validation_status = ValidationStatus.GOOD
            elif price_accuracy <= 5.0 and (direction_correct or high_low_bonus > 0):
                validation_status = ValidationStatus.GOOD
            elif price_accuracy <= 8.0 and direction_correct:
                validation_status = ValidationStatus.FAIR
            elif price_accuracy <= 10.0:
                validation_status = ValidationStatus.FAIR
            else:
                validation_status = ValidationStatus.POOR

            # åˆ›å»ºéªŒè¯ç»“æœ
            validation_result = ValidationResult(
                prediction_id=prediction_id,
                prediction_timestamp=prediction_timestamp,
                validation_timestamp=datetime.now(),
                predicted_price=predicted_price,
                actual_price=actual_price,
                actual_high=actual_high,
                actual_low=actual_low,
                price_error=price_error,
                price_error_pct=price_error_pct,
                predicted_direction=predicted_direction,
                actual_direction=actual_direction,
                direction_correct=direction_correct,
                high_prediction_correct=high_prediction_correct,
                low_prediction_correct=low_prediction_correct,
                confidence_score=prediction.get('volatility', 0),
                validation_status=validation_status,
                mae=mae,
                rmse=rmse,
                mape=mape,
                directional_accuracy=directional_accuracy,
                confidence_calibration=confidence_calibration + high_low_bonus * 0.1
            )
            
            # ä¿å­˜éªŒè¯ç»“æœ
            self._save_validation_result(validation_result)
            
            self.logger.info(f"é¢„æµ‹éªŒè¯å®Œæˆ - ID:{prediction_id}, "
                           f"ä»·æ ¼è¯¯å·®:{price_error:+.2f}({price_error_pct:+.2f}%), "
                           f"æ–¹å‘{'æ­£ç¡®' if direction_correct else 'é”™è¯¯'}")
            
            return validation_result
            
        except Exception as e:
            self.logger.error(f"éªŒè¯é¢„æµ‹å¤±è´¥: {e}")
            return None
    
    def _get_actual_kline_at_time(self, instrument: str, target_time: datetime) -> Optional[Dict]:
        """è·å–æŒ‡å®šæ—¶é—´ç‚¹çš„å®é™…Kçº¿æ•°æ®ï¼ˆåŒ…å«é«˜ä½ä»·ï¼‰"""
        try:
            from src.data.kline_storage import KlineStorageService

            # ä½¿ç”¨Kçº¿å­˜å‚¨æœåŠ¡è·å–å†å²æ•°æ®
            kline_service = KlineStorageService(self.config, self.db_path)

            # é¦–å…ˆå°è¯•ä»æ•°æ®åº“è·å–
            kline_data = kline_service.get_historical_kline_at_time(
                instrument=instrument,
                target_time=target_time,
                bar_size="1m",
                tolerance_minutes=10
            )

            if kline_data:
                self.logger.info(f"ä»æ•°æ®åº“è·å–åˆ°Kçº¿æ•°æ®: {target_time.strftime('%H:%M:%S')}")
                return kline_data

            # å¦‚æœæ•°æ®åº“ä¸­æ²¡æœ‰ï¼Œå°è¯•ä»APIè·å–å¹¶å­˜å‚¨
            self.logger.info(f"æ•°æ®åº“ä¸­æ— Kçº¿æ•°æ®ï¼Œå°è¯•ä»APIè·å–: {target_time.strftime('%H:%M:%S')}")

            # è·å–ç›®æ ‡æ—¶é—´å‰åçš„Kçº¿æ•°æ®
            current_time = datetime.now()
            start_time = target_time - timedelta(minutes=30)
            end_time = min(target_time + timedelta(minutes=30), current_time)

            df = self.data_fetcher.get_historical_klines(
                instrument=instrument,
                bar="1m",
                start_time=start_time.strftime('%Y-%m-%dT%H:%M:%SZ'),
                end_time=end_time.strftime('%Y-%m-%dT%H:%M:%SZ'),
                limit=60,
                validate_quality=False
            )

            if df.empty:
                self.logger.warning(f"æ— æ³•è·å–å†å²Kçº¿æ•°æ®: {instrument}")
                return None

            # å­˜å‚¨è·å–åˆ°çš„æ•°æ®
            kline_service._store_klines_to_db(df, instrument, "1m")

            # æ‰¾åˆ°æœ€æ¥è¿‘ç›®æ ‡æ—¶é—´çš„Kçº¿
            df['time_diff'] = abs((df['timestamps'] - target_time).dt.total_seconds())
            closest_row = df.loc[df['time_diff'].idxmin()]

            time_diff_seconds = closest_row['time_diff']
            self.logger.info(f"æ‰¾åˆ°æœ€æ¥è¿‘Kçº¿: {closest_row['timestamps']} (æ—¶é—´å·®: {time_diff_seconds:.0f}ç§’)")

            return {
                'timestamp': closest_row['timestamps'].isoformat(),
                'open': float(closest_row['open']),
                'high': float(closest_row['high']),
                'low': float(closest_row['low']),
                'close': float(closest_row['close']),
                'volume': float(closest_row['volume']),
                'amount': float(closest_row['amount'])
            }

        except Exception as e:
            self.logger.error(f"è·å–å®é™…Kçº¿æ•°æ®å¤±è´¥: {e}")
            return None
    
    def _calculate_actual_direction(self, start_price: float, end_price: float) -> str:
        """è®¡ç®—å®é™…ä»·æ ¼æ–¹å‘"""
        price_change_pct = (end_price - start_price) / start_price * 100
        
        if price_change_pct > 0.1:  # ä¸Šæ¶¨è¶…è¿‡0.1%
            return "up"
        elif price_change_pct < -0.1:  # ä¸‹è·Œè¶…è¿‡0.1%
            return "down"
        else:  # æ¨ªç›˜
            return "sideways"
    
    def _is_direction_correct(self, predicted: str, actual: str) -> bool:
        """åˆ¤æ–­æ–¹å‘é¢„æµ‹æ˜¯å¦æ­£ç¡®"""
        # æ–¹å‘æ˜ å°„
        up_directions = ['up', 'bullish']
        down_directions = ['down', 'bearish']
        neutral_directions = ['sideways', 'neutral']
        
        if predicted in up_directions and actual in up_directions:
            return True
        elif predicted in down_directions and actual in down_directions:
            return True
        elif predicted in neutral_directions and actual in neutral_directions:
            return True
        else:
            return False
    
    def _calculate_confidence_calibration(self, predicted: float, actual: float, volatility: float) -> float:
        """è®¡ç®—ç½®ä¿¡åº¦æ ¡å‡†åˆ†æ•°"""
        try:
            # åŸºäºæ³¢åŠ¨ç‡çš„æœŸæœ›è¯¯å·®èŒƒå›´
            expected_error_range = volatility * 2  # 2å€æ³¢åŠ¨ç‡ä½œä¸ºæœŸæœ›è¯¯å·®èŒƒå›´
            actual_error = abs(actual - predicted)
            
            if expected_error_range == 0:
                return 0.5  # é»˜è®¤å€¼
            
            # æ ¡å‡†åˆ†æ•°ï¼šå®é™…è¯¯å·®åœ¨æœŸæœ›èŒƒå›´å†…å¾—åˆ†è¾ƒé«˜
            calibration = max(0, 1 - (actual_error / expected_error_range))
            return min(1.0, calibration)
            
        except Exception as e:
            self.logger.error(f"è®¡ç®—ç½®ä¿¡åº¦æ ¡å‡†å¤±è´¥: {e}")
            return 0.5
    
    def _save_validation_result(self, result: ValidationResult):
        """ä¿å­˜éªŒè¯ç»“æœåˆ°æ•°æ®åº“"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                INSERT INTO prediction_validations (
                    prediction_id, validation_timestamp,
                    predicted_price, actual_price, actual_high, actual_low,
                    price_error, price_error_pct,
                    predicted_direction, actual_direction, direction_correct,
                    high_prediction_correct, low_prediction_correct,
                    validation_status, mae, rmse, mape,
                    directional_accuracy, confidence_calibration
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                result.prediction_id,
                result.validation_timestamp.isoformat(),
                result.predicted_price,
                result.actual_price,
                result.actual_high,
                result.actual_low,
                result.price_error,
                result.price_error_pct,
                result.predicted_direction,
                result.actual_direction,
                result.direction_correct,
                result.high_prediction_correct,
                result.low_prediction_correct,
                result.validation_status.value,
                result.mae,
                result.rmse,
                result.mape,
                result.directional_accuracy,
                result.confidence_calibration
            ))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜éªŒè¯ç»“æœå¤±è´¥: {e}")
            raise

    def _log_next_validation_time(self):
        """è®°å½•ä¸‹æ¬¡éªŒè¯æ—¶é—´"""
        try:
            conn = sqlite3.connect(self.db_path)

            # æŸ¥è¯¢æœ€è¿‘24å°æ—¶å†…çš„æœªéªŒè¯é¢„æµ‹
            cutoff_time = (datetime.now() - timedelta(hours=24)).isoformat()

            query = '''
                SELECT p.id, p.instrument, p.timestamp, p.pred_hours,
                       datetime(p.timestamp, '+' || p.pred_hours || ' hours') as target_time
                FROM predictions p
                LEFT JOIN prediction_validations pv ON p.id = pv.prediction_id
                WHERE pv.prediction_id IS NULL
                AND p.timestamp > ?
                ORDER BY p.timestamp ASC
                LIMIT 3
            '''

            df = pd.read_sql_query(query, conn, params=(cutoff_time,))
            conn.close()

            if not df.empty:
                current_time = datetime.now()
                found_next = False

                for _, row in df.iterrows():
                    target_time = datetime.fromisoformat(row['target_time'])
                    validation_start = target_time  # éªŒè¯çª—å£å¼€å§‹æ—¶é—´

                    if validation_start > current_time:
                        wait_minutes = (validation_start - current_time).total_seconds() / 60
                        instrument_short = row['instrument'].replace('-USDT-SWAP', '')
                        self.logger.info(f"ğŸ“… ä¸‹ä¸ªéªŒè¯: {instrument_short} (ID {row['id']}) "
                                       f"åœ¨ {validation_start.strftime('%H:%M:%S')} "
                                       f"(è¿˜éœ€ç­‰å¾… {wait_minutes:.0f} åˆ†é’Ÿ)")
                        found_next = True
                        break

                if not found_next:
                    self.logger.info("ğŸ“… æœ€è¿‘çš„é¢„æµ‹éƒ½å·²è¿‡éªŒè¯çª—å£")
            else:
                self.logger.info("ğŸ“… æš‚æ— æœ€è¿‘çš„å¾…éªŒè¯é¢„æµ‹")

        except Exception as e:
            self.logger.debug(f"è®°å½•ä¸‹æ¬¡éªŒè¯æ—¶é—´å¤±è´¥: {e}")

    def run_validation_cycle(self) -> Dict[str, any]:
        """è¿è¡Œä¸€æ¬¡éªŒè¯å‘¨æœŸ"""
        try:
            self.logger.info("ğŸ” å¼€å§‹é¢„æµ‹éªŒè¯å‘¨æœŸ")

            # è·å–å¾…éªŒè¯çš„é¢„æµ‹
            pending_predictions = self.get_pending_validations()

            if not pending_predictions:
                # æ£€æŸ¥æœ€è¿‘çš„é¢„æµ‹ä½•æ—¶å¯ä»¥éªŒè¯
                self._log_next_validation_time()
                self.logger.info("æš‚æ— å¾…éªŒè¯çš„é¢„æµ‹")
                return {"validated_count": 0, "results": []}

            self.logger.info(f"å‘ç° {len(pending_predictions)} ä¸ªå¾…éªŒè¯é¢„æµ‹")

            # éªŒè¯æ¯ä¸ªé¢„æµ‹
            validation_results = []
            successful_validations = 0

            for prediction in pending_predictions:
                result = self.validate_prediction(prediction)
                if result:
                    validation_results.append(result)
                    successful_validations += 1

            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            if validation_results:
                self._update_validation_statistics(validation_results)

            self.logger.info(f"âœ… éªŒè¯å‘¨æœŸå®Œæˆï¼ŒæˆåŠŸéªŒè¯ {successful_validations} ä¸ªé¢„æµ‹")

            return {
                "validated_count": successful_validations,
                "results": validation_results
            }

        except Exception as e:
            self.logger.error(f"âŒ éªŒè¯å‘¨æœŸæ‰§è¡Œå¤±è´¥: {e}")
            return {"validated_count": 0, "results": []}

    def _update_validation_statistics(self, results: List[ValidationResult]):
        """æ›´æ–°éªŒè¯ç»Ÿè®¡ä¿¡æ¯"""
        try:
            if not results:
                return

            # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
            total_predictions = len(results)
            avg_mae = np.mean([r.mae for r in results])
            avg_rmse = np.sqrt(np.mean([r.rmse for r in results]))
            avg_mape = np.mean([r.mape for r in results])
            directional_accuracy = np.mean([r.directional_accuracy for r in results]) * 100
            confidence_calibration = np.mean([r.confidence_calibration for r in results])

            # è®¡ç®—å¯é æ€§è¯„åˆ†ï¼ˆç»¼åˆæŒ‡æ ‡ï¼‰
            reliability_score = (
                (1 - min(avg_mape / 100, 1.0)) * 0.4 +  # ä»·æ ¼å‡†ç¡®æ€§æƒé‡40%
                (directional_accuracy / 100) * 0.4 +      # æ–¹å‘å‡†ç¡®æ€§æƒé‡40%
                confidence_calibration * 0.2              # ç½®ä¿¡åº¦æ ¡å‡†æƒé‡20%
            )

            # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()

            period_start = min(r.prediction_timestamp for r in results)
            period_end = max(r.prediction_timestamp for r in results)

            cursor.execute('''
                INSERT INTO validation_statistics (
                    period_start, period_end, total_predictions, validated_predictions,
                    avg_mae, avg_rmse, avg_mape, directional_accuracy,
                    confidence_calibration, reliability_score
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                period_start.isoformat(),
                period_end.isoformat(),
                total_predictions,
                total_predictions,
                avg_mae,
                avg_rmse,
                avg_mape,
                directional_accuracy,
                confidence_calibration,
                reliability_score
            ))

            conn.commit()
            conn.close()

            self.logger.info(f"ç»Ÿè®¡ä¿¡æ¯æ›´æ–°å®Œæˆ - å¯é æ€§è¯„åˆ†: {reliability_score:.2%}")

        except Exception as e:
            self.logger.error(f"æ›´æ–°éªŒè¯ç»Ÿè®¡å¤±è´¥: {e}")

    def get_validation_report(self, hours: int = 24) -> Dict[str, any]:
        """è·å–éªŒè¯æŠ¥å‘Š"""
        try:
            conn = sqlite3.connect(self.db_path)

            cutoff_time = (datetime.now() - timedelta(hours=hours)).isoformat()

            # è·å–éªŒè¯ç»“æœ
            query = '''
                SELECT * FROM prediction_validations
                WHERE validation_timestamp > ?
                ORDER BY validation_timestamp DESC
            '''

            df = pd.read_sql_query(query, conn, params=(cutoff_time,))

            if df.empty:
                conn.close()
                return {
                    "period_hours": hours,
                    "total_validations": 0,
                    "metrics": {},
                    "summary": "æš‚æ— éªŒè¯æ•°æ®"
                }

            # è®¡ç®—æŠ¥å‘ŠæŒ‡æ ‡
            total_validations = len(df)
            avg_mae = df['mae'].mean()
            avg_rmse = np.sqrt(df['rmse'].mean())
            avg_mape = df['mape'].mean()
            directional_accuracy = df['directional_accuracy'].mean() * 100
            confidence_calibration = df['confidence_calibration'].mean()

            # æ–¹å‘é¢„æµ‹åˆ†å¸ƒ
            direction_stats = df.groupby(['predicted_direction', 'direction_correct']).size().unstack(fill_value=0)

            # è¯¯å·®åˆ†å¸ƒ
            error_percentiles = df['price_error_pct'].describe()

            conn.close()

            return {
                "period_hours": hours,
                "total_validations": total_validations,
                "metrics": {
                    "avg_mae": avg_mae,
                    "avg_rmse": avg_rmse,
                    "avg_mape": avg_mape,
                    "directional_accuracy": directional_accuracy,
                    "confidence_calibration": confidence_calibration
                },
                "direction_stats": direction_stats.to_dict() if not direction_stats.empty else {},
                "error_distribution": error_percentiles.to_dict(),
                "summary": f"è¿‡å»{hours}å°æ—¶éªŒè¯äº†{total_validations}ä¸ªé¢„æµ‹ï¼Œ"
                          f"æ–¹å‘å‡†ç¡®ç‡{directional_accuracy:.1f}%ï¼Œ"
                          f"å¹³å‡ä»·æ ¼è¯¯å·®{avg_mape:.2f}%"
            }

        except Exception as e:
            self.logger.error(f"è·å–éªŒè¯æŠ¥å‘Šå¤±è´¥: {e}")
            return {"error": str(e)}

    def get_model_performance_trend(self, days: int = 7) -> Dict[str, any]:
        """è·å–æ¨¡å‹æ€§èƒ½è¶‹åŠ¿"""
        try:
            conn = sqlite3.connect(self.db_path)

            cutoff_time = (datetime.now() - timedelta(days=days)).isoformat()

            query = '''
                SELECT DATE(validation_timestamp) as date,
                       AVG(mae) as avg_mae,
                       AVG(mape) as avg_mape,
                       AVG(directional_accuracy) * 100 as directional_accuracy,
                       COUNT(*) as validation_count
                FROM prediction_validations
                WHERE validation_timestamp > ?
                GROUP BY DATE(validation_timestamp)
                ORDER BY date ASC
            '''

            df = pd.read_sql_query(query, conn, params=(cutoff_time,))
            conn.close()

            if df.empty:
                return {"trend_data": [], "summary": "æš‚æ— è¶‹åŠ¿æ•°æ®"}

            trend_data = df.to_dict('records')

            # è®¡ç®—è¶‹åŠ¿
            if len(df) > 1:
                mae_trend = "æ”¹å–„" if df['avg_mae'].iloc[-1] < df['avg_mae'].iloc[0] else "æ¶åŒ–"
                accuracy_trend = "æå‡" if df['directional_accuracy'].iloc[-1] > df['directional_accuracy'].iloc[0] else "ä¸‹é™"
            else:
                mae_trend = "ç¨³å®š"
                accuracy_trend = "ç¨³å®š"

            return {
                "trend_data": trend_data,
                "summary": f"è¿‡å»{days}å¤©æ¨¡å‹æ€§èƒ½è¶‹åŠ¿ï¼šä»·æ ¼é¢„æµ‹{mae_trend}ï¼Œæ–¹å‘é¢„æµ‹{accuracy_trend}"
            }

        except Exception as e:
            self.logger.error(f"è·å–æ€§èƒ½è¶‹åŠ¿å¤±è´¥: {e}")
            return {"error": str(e)}
