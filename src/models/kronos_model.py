"""
Kronosæ¨¡åž‹é›†æˆæ¨¡å—
åŸºäºŽshiyu-coder/Kronosé¡¹ç›®çš„é¢„æµ‹åŠŸèƒ½
"""
import os
import sys
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
import logging
import json
from datetime import datetime, timedelta
from typing import Optional, Tuple
from pathlib import Path

# æ·»åŠ æ¨¡åž‹è·¯å¾„åˆ°ç³»ç»Ÿè·¯å¾„
current_dir = Path(__file__).parent
project_root = current_dir.parent.parent
sys.path.append(str(project_root))

# å¯¼å…¥Kronosç›¸å…³æ¨¡å—
try:
    from huggingface_hub import PyTorchModelHubMixin
    from tqdm import trange
    import math
    from einops import rearrange, reduce
    from torch.autograd import Function
except ImportError:
    print("è¯·å®‰è£…ä¾èµ–: pip install huggingface_hub tqdm einops")
    raise


# å¯¼å…¥Kronosæ¨¡å—
from .kronos_modules import (
    TransformerBlock, HierarchicalEmbedding, DependencyAwareLayer,
    DualHead, TemporalEmbedding, RMSNorm
)
from .quantizer import BSQuantizer


class KronosTokenizer(nn.Module, PyTorchModelHubMixin):
    """Kronos Tokenizeræ¨¡å—"""

    def __init__(self, d_in, d_model, n_heads, ff_dim, n_enc_layers, n_dec_layers,
                 ffn_dropout_p, attn_dropout_p, resid_dropout_p, s1_bits, s2_bits,
                 beta, gamma0, gamma, zeta, group_size):
        super().__init__()
        self.d_in = d_in
        self.d_model = d_model
        self.n_heads = n_heads
        self.ff_dim = ff_dim
        self.enc_layers = n_enc_layers
        self.dec_layers = n_dec_layers
        self.ffn_dropout_p = ffn_dropout_p
        self.attn_dropout_p = attn_dropout_p
        self.resid_dropout_p = resid_dropout_p

        self.s1_bits = s1_bits
        self.s2_bits = s2_bits
        self.codebook_dim = s1_bits + s2_bits
        self.embed = nn.Linear(self.d_in, self.d_model)
        self.head = nn.Linear(self.d_model, self.d_in)

        # Encoderå’ŒDecoder Transformer Blocks
        self.encoder = nn.ModuleList([
            TransformerBlock(self.d_model, self.n_heads, self.ff_dim, self.ffn_dropout_p, self.attn_dropout_p, self.resid_dropout_p)
            for _ in range(self.enc_layers - 1)
        ])
        self.decoder = nn.ModuleList([
            TransformerBlock(self.d_model, self.n_heads, self.ff_dim, self.ffn_dropout_p, self.attn_dropout_p, self.resid_dropout_p)
            for _ in range(self.dec_layers - 1)
        ])
        self.quant_embed = nn.Linear(in_features=self.d_model, out_features=self.codebook_dim)
        self.post_quant_embed_pre = nn.Linear(in_features=self.s1_bits, out_features=self.d_model)
        self.post_quant_embed = nn.Linear(in_features=self.codebook_dim, out_features=self.d_model)
        self.tokenizer = BSQuantizer(self.s1_bits, self.s2_bits, beta, gamma0, gamma, zeta, group_size)

    def forward(self, x):
        z = self.embed(x)

        for layer in self.encoder:
            z = layer(z)

        z = self.quant_embed(z)
        bsq_loss, quantized, z_indices = self.tokenizer(z)

        quantized_pre = quantized[:, :, :self.s1_bits]
        z_pre = self.post_quant_embed_pre(quantized_pre)
        z = self.post_quant_embed(quantized)

        for layer in self.decoder:
            z_pre = layer(z_pre)
        z_pre = self.head(z_pre)

        for layer in self.decoder:
            z = layer(z)
        z = self.head(z)

        return (z_pre, z), bsq_loss, quantized, z_indices

    def indices_to_bits(self, x, half=False):
        if half:
            x1, x2 = x[0], x[1]
            mask = 2 ** torch.arange(self.codebook_dim//2, device=x1.device, dtype=torch.long)
            x1 = (x1.unsqueeze(-1) & mask) != 0
            x2 = (x2.unsqueeze(-1) & mask) != 0
            x = torch.cat([x1, x2], dim=-1)
        else:
            mask = 2 ** torch.arange(self.codebook_dim, device=x.device, dtype=torch.long)
            x = (x.unsqueeze(-1) & mask) != 0

        x = x.float() * 2 - 1
        q_scale = 1. / (self.codebook_dim ** 0.5)
        x = x * q_scale
        return x

    def encode(self, x, half=False):
        z = self.embed(x)
        for layer in self.encoder:
            z = layer(z)
        z = self.quant_embed(z)
        bsq_loss, quantized, z_indices = self.tokenizer(z, half)
        return z_indices

    def decode(self, x, half=False):
        quantized = self.indices_to_bits(x, half)
        z = self.post_quant_embed(quantized)
        for layer in self.decoder:
            z = layer(z)
        z = self.head(z)
        return z


class Kronos(nn.Module, PyTorchModelHubMixin):
    """Kronosä¸»æ¨¡åž‹"""

    def __init__(self, s1_bits, s2_bits, n_layers, d_model, n_heads, ff_dim,
                 ffn_dropout_p, attn_dropout_p, resid_dropout_p, token_dropout_p, learn_te):
        super().__init__()
        self.s1_bits = s1_bits
        self.s2_bits = s2_bits
        self.n_layers = n_layers
        self.d_model = d_model
        self.n_heads = n_heads
        self.learn_te = learn_te
        self.ff_dim = ff_dim
        self.ffn_dropout_p = ffn_dropout_p
        self.attn_dropout_p = attn_dropout_p
        self.resid_dropout_p = resid_dropout_p
        self.token_dropout_p = token_dropout_p

        self.s1_vocab_size = 2 ** self.s1_bits
        self.token_drop = nn.Dropout(self.token_dropout_p)
        self.embedding = HierarchicalEmbedding(self.s1_bits, self.s2_bits, self.d_model)
        self.time_emb = TemporalEmbedding(self.d_model, self.learn_te)
        self.transformer = nn.ModuleList([
            TransformerBlock(self.d_model, self.n_heads, self.ff_dim, self.ffn_dropout_p, self.attn_dropout_p, self.resid_dropout_p)
            for _ in range(self.n_layers)
        ])
        self.norm = RMSNorm(self.d_model)
        self.dep_layer = DependencyAwareLayer(self.d_model)
        self.head = DualHead(self.s1_bits, self.s2_bits, self.d_model)
        self.apply(self._init_weights)

    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            nn.init.xavier_normal_(module.weight)
            if module.bias is not None:
                nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            nn.init.normal_(module.weight, mean=0, std=self.embedding.d_model ** -0.5)
        elif isinstance(module, nn.LayerNorm):
            nn.init.ones_(module.weight)
            nn.init.zeros_(module.bias)
        elif isinstance(module, RMSNorm):
            nn.init.ones_(module.weight)

    def decode_s1(self, s1_ids, s2_ids, stamp=None, padding_mask=None):
        x = self.embedding([s1_ids, s2_ids])
        if stamp is not None:
            time_embedding = self.time_emb(stamp)
            x = x + time_embedding
        x = self.token_drop(x)

        for layer in self.transformer:
            x = layer(x, key_padding_mask=padding_mask)

        x = self.norm(x)
        s1_logits = self.head(x)
        return s1_logits, x

    def decode_s2(self, context, s1_ids, padding_mask=None):
        sibling_embed = self.embedding.emb_s1(s1_ids)
        x2 = self.dep_layer(context, sibling_embed, key_padding_mask=padding_mask)
        return self.head.cond_forward(x2)


class KronosPredictor:
    """Kronosé¢„æµ‹å™¨ï¼Œç”¨äºŽBTC-USDTæ°¸ç»­åˆçº¦ä»·æ ¼é¢„æµ‹"""

    def __init__(self, model_path: str = None, tokenizer_path: str = None, device: str = "auto", auto_download: bool = True):
        """
        åˆå§‹åŒ–Kronosé¢„æµ‹å™¨

        Args:
            model_path: æ¨¡åž‹è·¯å¾„ï¼Œé»˜è®¤ä½¿ç”¨é¡¹ç›®ä¸­çš„kronos-small
            tokenizer_path: tokenizerè·¯å¾„ï¼Œé»˜è®¤ä½¿ç”¨é¡¹ç›®ä¸­çš„tokenizer
            device: è®¡ç®—è®¾å¤‡ï¼Œauto/cpu/cuda/mps
            auto_download: æ˜¯å¦è‡ªåŠ¨ä¸‹è½½ç¼ºå¤±çš„æ¨¡åž‹
        """
        self.logger = logging.getLogger(__name__)
        self.device = self._get_optimal_device(device)

        # è®¾ç½®é»˜è®¤è·¯å¾„
        if model_path is None:
            model_path = project_root / "models" / "kronos-small"
        if tokenizer_path is None:
            tokenizer_path = project_root / "models" / "tokenizer"

        self.model_path = Path(model_path)
        self.tokenizer_path = Path(tokenizer_path)

        # æ£€æŸ¥æ¨¡åž‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æžœä¸å­˜åœ¨ä¸”å…è®¸è‡ªåŠ¨ä¸‹è½½ï¼Œåˆ™ä¸‹è½½
        if not self._check_models_exist():
            if auto_download:
                self.logger.info("ðŸ“¥ æ¨¡åž‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå¼€å§‹è‡ªåŠ¨ä¸‹è½½...")
                if not self._download_models():
                    raise FileNotFoundError(f"æ¨¡åž‹ä¸‹è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿žæŽ¥")
            else:
                raise FileNotFoundError(f"æ¨¡åž‹è·¯å¾„ä¸å­˜åœ¨: {self.model_path}, Tokenizerè·¯å¾„ä¸å­˜åœ¨: {self.tokenizer_path}")

        self.logger.info(f"ä½¿ç”¨æ¨¡åž‹è·¯å¾„: {self.model_path}")
        self.logger.info(f"ä½¿ç”¨tokenizerè·¯å¾„: {self.tokenizer_path}")

        # åˆå§‹åŒ–æ¨¡åž‹å’Œtokenizer
        self._load_models()

        # é¢„æµ‹ç›¸å…³å‚æ•°
        self.max_context = 512
        self.clip = 5
        self.price_cols = ['open', 'high', 'low', 'close']
        self.vol_col = 'volume'
        self.amt_col = 'amount'
        self.time_cols = ['minute', 'hour', 'weekday', 'day', 'month']

        self.logger.info(f"ðŸ–¥ï¸ ä½¿ç”¨è®¡ç®—è®¾å¤‡: {self.device}")

    def _check_models_exist(self) -> bool:
        """æ£€æŸ¥æ¨¡åž‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
        model_exists = (self.model_path.exists() and
                       (self.model_path / "config.json").exists() and
                       (self.model_path / "model.safetensors").exists())

        tokenizer_exists = (self.tokenizer_path.exists() and
                           (self.tokenizer_path / "config.json").exists() and
                           (self.tokenizer_path / "model.safetensors").exists())

        return model_exists and tokenizer_exists

    def _download_models(self) -> bool:
        """ä¸‹è½½æ¨¡åž‹æ–‡ä»¶"""
        try:
            # å¯¼å…¥ä¸‹è½½å‡½æ•°
            from .download_models import download_kronos_models

            # æ‰§è¡Œä¸‹è½½ï¼Œä¼ å…¥modelsç›®å½•è·¯å¾„
            models_dir = self.model_path.parent
            success = download_kronos_models(str(models_dir))

            if success:
                self.logger.info("âœ… æ¨¡åž‹ä¸‹è½½å®Œæˆ")
                return True
            else:
                self.logger.error("âŒ æ¨¡åž‹ä¸‹è½½å¤±è´¥")
                return False

        except Exception as e:
            self.logger.error(f"âŒ æ¨¡åž‹ä¸‹è½½å¼‚å¸¸: {e}")
            return False

    def _get_optimal_device(self, device: str) -> str:
        """èŽ·å–æœ€ä¼˜è®¡ç®—è®¾å¤‡"""
        if device == "auto":
            # è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜è®¾å¤‡
            if torch.backends.mps.is_available():
                return "mps"  # M1/M2 Mac GPU
            elif torch.cuda.is_available():
                return "cuda"  # NVIDIA GPU
            else:
                return "cpu"
        elif device == "mps":
            if torch.backends.mps.is_available():
                return "mps"
            else:
                self.logger.warning("âš ï¸ MPSä¸å¯ç”¨ï¼Œå›žé€€åˆ°CPU")
                return "cpu"
        elif device == "cuda":
            if torch.cuda.is_available():
                return "cuda"
            else:
                self.logger.warning("âš ï¸ CUDAä¸å¯ç”¨ï¼Œå›žé€€åˆ°CPU")
                return "cpu"
        else:
            return "cpu"

    def _load_models(self):
        """åŠ è½½æ¨¡åž‹å’Œtokenizer"""
        try:
            self.logger.info("æ­£åœ¨åŠ è½½Kronosæ¨¡åž‹...")

            # åŠ è½½tokenizeré…ç½®å’Œæƒé‡
            tokenizer_config_path = self.tokenizer_path / "config.json"
            with open(tokenizer_config_path, 'r') as f:
                tokenizer_config = json.load(f)

            # åˆ›å»ºtokenizer
            self.tokenizer = KronosTokenizer(
                d_in=tokenizer_config['d_in'],
                d_model=tokenizer_config['d_model'],
                n_heads=tokenizer_config['n_heads'],
                ff_dim=tokenizer_config['ff_dim'],
                n_enc_layers=tokenizer_config['n_enc_layers'],
                n_dec_layers=tokenizer_config['n_dec_layers'],
                ffn_dropout_p=tokenizer_config['ffn_dropout_p'],
                attn_dropout_p=tokenizer_config['attn_dropout_p'],
                resid_dropout_p=tokenizer_config['resid_dropout_p'],
                s1_bits=tokenizer_config['s1_bits'],
                s2_bits=tokenizer_config['s2_bits'],
                beta=tokenizer_config['beta'],
                gamma0=tokenizer_config['gamma0'],
                gamma=tokenizer_config['gamma'],
                zeta=tokenizer_config['zeta'],
                group_size=tokenizer_config['group_size']
            )

            # åŠ è½½tokenizeræƒé‡
            tokenizer_weights_path = self.tokenizer_path / "model.safetensors"
            if tokenizer_weights_path.exists():
                from safetensors.torch import load_file
                tokenizer_state_dict = load_file(tokenizer_weights_path)
                self.tokenizer.load_state_dict(tokenizer_state_dict)

            # åŠ è½½æ¨¡åž‹é…ç½®å’Œæƒé‡
            model_config_path = self.model_path / "config.json"
            with open(model_config_path, 'r') as f:
                model_config = json.load(f)

            # åˆ›å»ºæ¨¡åž‹
            self.model = Kronos(
                s1_bits=model_config['s1_bits'],
                s2_bits=model_config['s2_bits'],
                n_layers=model_config['n_layers'],
                d_model=model_config['d_model'],
                n_heads=model_config['n_heads'],
                ff_dim=model_config['ff_dim'],
                ffn_dropout_p=model_config['ffn_dropout_p'],
                attn_dropout_p=model_config['attn_dropout_p'],
                resid_dropout_p=model_config['resid_dropout_p'],
                token_dropout_p=model_config['token_dropout_p'],
                learn_te=model_config['learn_te']
            )

            # åŠ è½½æ¨¡åž‹æƒé‡
            model_weights_path = self.model_path / "model.safetensors"
            if model_weights_path.exists():
                from safetensors.torch import load_file
                model_state_dict = load_file(model_weights_path)
                self.model.load_state_dict(model_state_dict)

            # ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
            self.tokenizer = self.tokenizer.to(self.device)
            self.model = self.model.to(self.device)

            # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
            self.tokenizer.eval()
            self.model.eval()

            self.logger.info("âœ… Kronosæ¨¡åž‹åŠ è½½æˆåŠŸ")

        except Exception as e:
            self.logger.error(f"âŒ æ¨¡åž‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def _calc_time_stamps(self, timestamp_series: pd.Series) -> pd.DataFrame:
        """è®¡ç®—æ—¶é—´æˆ³ç‰¹å¾"""
        time_df = pd.DataFrame()
        time_df['minute'] = timestamp_series.dt.minute
        time_df['hour'] = timestamp_series.dt.hour
        time_df['weekday'] = timestamp_series.dt.weekday
        time_df['day'] = timestamp_series.dt.day
        time_df['month'] = timestamp_series.dt.month
        return time_df
    
    def _prepare_data(self, df: pd.DataFrame, timestamp_series: pd.Series) -> Tuple[np.ndarray, np.ndarray]:
        """å‡†å¤‡è¾“å…¥æ•°æ®"""
        # æ£€æŸ¥å¿…è¦çš„åˆ—
        if not all(col in df.columns for col in self.price_cols):
            raise ValueError(f"æ•°æ®ç¼ºå°‘å¿…è¦çš„ä»·æ ¼åˆ—: {self.price_cols}")
        
        # å¤„ç†ç¼ºå¤±çš„volumeå’Œamountåˆ—
        df = df.copy()
        if self.vol_col not in df.columns:
            df[self.vol_col] = 0.0
        if self.amt_col not in df.columns:
            df[self.amt_col] = df[self.vol_col] * df[self.price_cols].mean(axis=1)
        
        # æ£€æŸ¥NaNå€¼
        if df[self.price_cols + [self.vol_col, self.amt_col]].isnull().values.any():
            raise ValueError("è¾“å…¥æ•°æ®åŒ…å«NaNå€¼")
        
        # å‡†å¤‡ä»·æ ¼å’Œæˆäº¤é‡æ•°æ®
        x = df[self.price_cols + [self.vol_col, self.amt_col]].values.astype(np.float32)
        
        # å‡†å¤‡æ—¶é—´ç‰¹å¾
        time_df = self._calc_time_stamps(timestamp_series)
        x_stamp = time_df.values.astype(np.float32)
        
        return x, x_stamp
    
    def predict(self, df: pd.DataFrame, x_timestamp: pd.Series, y_timestamp: pd.Series,
                pred_len: int = 24, temperature: float = 1.0, top_p: float = 0.9,
                sample_count: int = 1, verbose: bool = True, seed: Optional[int] = None,
                deterministic: bool = False) -> pd.DataFrame:
        """
        è¿›è¡Œä»·æ ¼é¢„æµ‹

        Args:
            df: åŽ†å²Kçº¿æ•°æ®ï¼ŒåŒ…å«['open', 'high', 'low', 'close']åˆ—
            x_timestamp: åŽ†å²æ•°æ®å¯¹åº”çš„æ—¶é—´æˆ³
            y_timestamp: é¢„æµ‹æ—¶é—´æˆ³
            pred_len: é¢„æµ‹é•¿åº¦
            temperature: é‡‡æ ·æ¸©åº¦ï¼ˆè¶Šä½Žè¶Šç¡®å®šæ€§ï¼Œè¶Šé«˜è¶Šéšæœºï¼‰
            top_p: nucleusé‡‡æ ·å‚æ•°ï¼ˆ0.9è¡¨ç¤ºåªè€ƒè™‘ç´¯ç§¯æ¦‚çŽ‡90%çš„tokenï¼‰
            sample_count: é‡‡æ ·æ¬¡æ•°ï¼ˆå¤šæ¬¡é‡‡æ ·å–å¹³å‡ï¼‰
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
            seed: éšæœºç§å­ï¼ˆè®¾ç½®åŽç»“æžœå¯é‡çŽ°ï¼‰
            deterministic: æ˜¯å¦ä½¿ç”¨ç¡®å®šæ€§æ¨¡å¼ï¼ˆå‡å°‘éšæœºæ€§ï¼‰

        Returns:
            é¢„æµ‹ç»“æžœDataFrame
        """
        try:
            # è®¾ç½®éšæœºç§å­ï¼ˆå¦‚æžœæŒ‡å®šï¼‰
            if seed is not None:
                torch.manual_seed(seed)
                np.random.seed(seed)
                if torch.cuda.is_available():
                    torch.cuda.manual_seed(seed)
                if torch.backends.mps.is_available():
                    torch.mps.manual_seed(seed)
                self.logger.info(f"ðŸŽ² è®¾ç½®éšæœºç§å­: {seed}")

            # ç¡®å®šæ€§æ¨¡å¼è®¾ç½®
            if deterministic:
                temperature = 0.1  # é™ä½Žæ¸©åº¦å¢žåŠ ç¡®å®šæ€§
                top_p = 0.5       # å‡å°‘é‡‡æ ·èŒƒå›´
                self.logger.info("ðŸ”’ ä½¿ç”¨ç¡®å®šæ€§æ¨¡å¼ï¼ˆä½Žæ¸©åº¦é‡‡æ ·ï¼‰")

            self.logger.info(f"å¼€å§‹è¿›è¡ŒKronosé¢„æµ‹ï¼Œé¢„æµ‹é•¿åº¦: {pred_len}")
            self.logger.info(f"ðŸŒ¡ï¸ é‡‡æ ·å‚æ•° - æ¸©åº¦: {temperature}, Top-p: {top_p}, é‡‡æ ·æ¬¡æ•°: {sample_count}")

            # æ£€æŸ¥å¿…è¦çš„åˆ—
            if not all(col in df.columns for col in self.price_cols):
                raise ValueError(f"æ•°æ®ç¼ºå°‘å¿…è¦çš„ä»·æ ¼åˆ—: {self.price_cols}")

            # å¤„ç†ç¼ºå¤±çš„volumeå’Œamountåˆ—
            df = df.copy()
            if self.vol_col not in df.columns:
                df[self.vol_col] = 0.0
            if self.amt_col not in df.columns:
                df[self.amt_col] = df[self.vol_col] * df[self.price_cols].mean(axis=1)

            # æ£€æŸ¥NaNå€¼
            if df[self.price_cols + [self.vol_col, self.amt_col]].isnull().values.any():
                raise ValueError("è¾“å…¥æ•°æ®åŒ…å«NaNå€¼")

            # å‡†å¤‡æ—¶é—´ç‰¹å¾
            x_time_df = self._calc_time_stamps(x_timestamp)
            y_time_df = self._calc_time_stamps(y_timestamp)

            # å‡†å¤‡æ•°æ®
            x = df[self.price_cols + [self.vol_col, self.amt_col]].values.astype(np.float32)
            x_stamp = x_time_df.values.astype(np.float32)
            y_stamp = y_time_df.values.astype(np.float32)

            # æ•°æ®æ ‡å‡†åŒ–
            x_mean, x_std = np.mean(x, axis=0), np.std(x, axis=0)
            x_normalized = (x - x_mean) / (x_std + 1e-5)
            x_normalized = np.clip(x_normalized, -self.clip, self.clip)

            # è½¬æ¢ä¸ºtensorå¹¶æ·»åŠ batchç»´åº¦
            x_tensor = torch.from_numpy(x_normalized[np.newaxis, :]).to(self.device)
            x_stamp_tensor = torch.from_numpy(x_stamp[np.newaxis, :]).to(self.device)
            y_stamp_tensor = torch.from_numpy(y_stamp[np.newaxis, :]).to(self.device)

            # ä½¿ç”¨çœŸæ­£çš„Kronosæ¨¡åž‹è¿›è¡Œé¢„æµ‹
            preds = self._auto_regressive_inference(
                x_tensor, x_stamp_tensor, y_stamp_tensor,
                pred_len, temperature, 0, top_p, sample_count, verbose
            )

            # ç§»é™¤batchç»´åº¦å¹¶è½¬æ¢ä¸ºnumpy
            preds = preds.squeeze(0)

            # åæ ‡å‡†åŒ–
            preds = preds * (x_std + 1e-5) + x_mean

            # åˆ›å»ºé¢„æµ‹ç»“æžœDataFrame
            pred_df = pd.DataFrame(
                preds,
                columns=self.price_cols + [self.vol_col, self.amt_col],
                index=y_timestamp
            )

            self.logger.info("âœ… Kronosé¢„æµ‹å®Œæˆ")
            return pred_df

        except Exception as e:
            self.logger.error(f"âŒ é¢„æµ‹å¤±è´¥: {e}")
            raise
    
    def _top_k_top_p_filtering(self, logits, top_k: int = 0, top_p: float = 1.0,
                              filter_value: float = -float("Inf"), min_tokens_to_keep: int = 1):
        """Top-kå’Œnucleus (top-p)è¿‡æ»¤"""
        if top_k > 0:
            top_k = min(max(top_k, min_tokens_to_keep), logits.size(-1))
            indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]
            logits[indices_to_remove] = filter_value
            return logits

        if top_p < 1.0:
            sorted_logits, sorted_indices = torch.sort(logits, descending=True)
            cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)

            sorted_indices_to_remove = cumulative_probs > top_p
            if min_tokens_to_keep > 1:
                sorted_indices_to_remove[..., :min_tokens_to_keep] = 0
            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
            sorted_indices_to_remove[..., 0] = 0

            indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)
            logits[indices_to_remove] = filter_value
            return logits

    def _sample_from_logits(self, logits, temperature=1.0, top_k=None, top_p=None, sample_logits=True):
        """ä»Žlogitsä¸­é‡‡æ ·"""
        logits = logits / temperature
        if top_k is not None or top_p is not None:
            if top_k > 0 or top_p < 1.0:
                logits = self._top_k_top_p_filtering(logits, top_k=top_k, top_p=top_p)

        probs = F.softmax(logits, dim=-1)

        if not sample_logits:
            _, x = torch.topk(probs, k=1, dim=-1)
        else:
            x = torch.multinomial(probs, num_samples=1)

        return x

    def _auto_regressive_inference(self, x, x_stamp, y_stamp, pred_len, temperature=1.0,
                                  top_k=0, top_p=0.99, sample_count=1, verbose=False):
        """è‡ªå›žå½’æŽ¨ç†"""
        with torch.no_grad():
            batch_size = x.size(0)
            initial_seq_len = x.size(1)
            x = torch.clip(x, -self.clip, self.clip)

            device = x.device
            x = x.unsqueeze(1).repeat(1, sample_count, 1, 1).reshape(-1, x.size(1), x.size(2)).to(device)
            x_stamp = x_stamp.unsqueeze(1).repeat(1, sample_count, 1, 1).reshape(-1, x_stamp.size(1), x_stamp.size(2)).to(device)
            y_stamp = y_stamp.unsqueeze(1).repeat(1, sample_count, 1, 1).reshape(-1, y_stamp.size(1), y_stamp.size(2)).to(device)

            # ç¼–ç è¾“å…¥æ•°æ®
            x_token = self.tokenizer.encode(x, half=True)

            def get_dynamic_stamp(x_stamp, y_stamp, current_seq_len, pred_step):
                if current_seq_len <= self.max_context - pred_step:
                    return torch.cat([x_stamp, y_stamp[:, :pred_step, :]], dim=1)
                else:
                    start_idx = self.max_context - pred_step
                    return torch.cat([x_stamp[:, -start_idx:, :], y_stamp[:, :pred_step, :]], dim=1)

            if verbose:
                try:
                    from tqdm import trange
                    ran = trange
                except ImportError:
                    ran = range
            else:
                ran = range

            for i in ran(pred_len):
                current_seq_len = initial_seq_len + i

                if current_seq_len <= self.max_context:
                    input_tokens = x_token
                else:
                    input_tokens = [t[:, -self.max_context:].contiguous() for t in x_token]

                current_stamp = get_dynamic_stamp(x_stamp, y_stamp, current_seq_len, i)

                # è§£ç s1
                s1_logits, context = self.model.decode_s1(input_tokens[0], input_tokens[1], current_stamp)
                s1_logits = s1_logits[:, -1, :]
                sample_pre = self._sample_from_logits(s1_logits, temperature=temperature, top_k=top_k, top_p=top_p, sample_logits=True)

                # è§£ç s2
                s2_logits = self.model.decode_s2(context, sample_pre)
                s2_logits = s2_logits[:, -1, :]
                sample_post = self._sample_from_logits(s2_logits, temperature=temperature, top_k=top_k, top_p=top_p, sample_logits=True)

                # æ›´æ–°tokenåºåˆ—
                x_token[0] = torch.cat([x_token[0], sample_pre], dim=1)
                x_token[1] = torch.cat([x_token[1], sample_post], dim=1)

            # è§£ç æœ€ç»ˆç»“æžœ
            input_tokens = [t[:, -self.max_context:].contiguous() for t in x_token]
            z = self.tokenizer.decode(input_tokens, half=True)
            z = z.reshape(batch_size, sample_count, z.size(1), z.size(2))
            preds = z.cpu().numpy()
            preds = np.mean(preds, axis=1)

            return preds[:, -pred_len:, :]


def create_kronos_predictor(device: str = "cpu") -> KronosPredictor:
    """åˆ›å»ºKronosé¢„æµ‹å™¨å®žä¾‹"""
    return KronosPredictor(device=device)
